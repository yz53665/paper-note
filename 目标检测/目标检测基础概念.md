## 检测指标
+ 准确率（Accuracy）：正确识别比例
+ 精度（Precision）：$\frac{TP}{TP+FP}$，预测为正类中正确率（相反是漏检率）
+ 召回率（Recall）：$\frac{TP}{TP+FN}$，所有正类中找出了多少（相反是误检率）
+ F1值：综合考虑精度和召回率的指标
+ 平均精度均值(mAP)：由每个类别的精度-召回曲线计算。
+ IoU：检测框与真实框之间的相交面积占两者并集面积，衡量定位准确性
+ Top1(Accuracy)是指排名第一的类别与实际结果相符的准确率
+ Top5()是指排名前5包含实际结果的准确率

##### 1. mAP计算方式
每个类别PR曲线下面积AUC的平均值，越接近1性能越好。
PR曲线是设置不同判断的阈值时，模型预测结果的精度和召回率绘制的平面曲线
##### 2. 置信度分数
由两部分组成：
+ 框框得分（box score）：表示该框与目标边界框的重合程度，通常是计算IOU来评估
+ 物体得分（object score）：存在某个物体的概率。一般将每个预测框最高得分看作是该物体得分
1. Faster R-CNN中，框得分是回归网络得到，物体得分由分类网络得到
2. YOLO中，模型直接输出
3. RetinaNet，Focal loss计算物体得分
作用：
1. 用于判断预测结果是否准确
2. 用于筛选置信度高的预测框

##### 3. IOU
一般IoU大于0.5或0.7就认为正确定位了目标

##### 4. Ap@50
召回率在50%时的平均精度值（AP）

### 3D目标检测指标
除了上述指标，还有：
+ 平均平移误差（Average Translation error, ATE）二维欧式中心距离
+ 平均尺度误差（average scale error, ASE）1-IoU，IoU是角度对齐后的三位交并
+ 平均角度误差（average orientation error）是预测值和真实值之间最小的偏航角差
+ 平均速度误差（average velocity error，AVE）二维速度差的L2范数（m/s）
+ 平均属性错误（average attribute error，AAE）1-acc，acc是分类准确度

## 后处理
##### 前处理
+ 图像预处理：缩放、归一化、旋转、裁剪，提高模型鲁棒性和准确性
+ 特征提取：
+ 数据增强：随机裁剪、随机反转、亮度调节、添加噪声，增加数据量，提高模型泛化性
##### 后处理
+ 非极大值抑制（NMS）：对目标框去重和筛选，只保留得分最高的，并抑制周围高度重叠的框
+ 筛选：判断目标是非符合检测条件
+ 矫正：对检测框位置和倾斜角度矫正，提高识别准确性

#### 1. NMS（Non-Maximum suppression）
以最大置信度的框为基准，抑制其它的框
1. 对检测框按置信度分数排序
2. 取得分最高的框A，从候选集里面删除A
3. 按重叠率大小判断剩余是否和A重叠，存在则计算IoU
4. IoU大于阈值，删除重叠框

#### 2. NMS改进
1. Soft-NMS：重叠框的分数会随着IoU增加而减少，而不是直接删掉。使模型更加关注可靠的结果，避免错判。
2. Adaptive NMS：解决不同IoU阈值不适应与不同物体大小和密度。引入自适应参数，根据候选框大小和样例分布自适应调整IoU阈值
3. 可学习权重的Soft-NMS：引入学习权重机端任务，学习如何选择重叠框

#### 3. 不需要后处理的目标检测算法
##### 3.1 YOLO
单阶段模型。图像分成多个网格，每个网格预测N个边界框及置信度，筛选最佳的框。
##### 3.2 SSD
类似YOLO多网格处理，但采用预设先验框，让模型更有效学习各个物体形状和大小

## Anchor
### 1. 目标检测分类方法
#### 1.1 Anchor-free vs Anchor-based:
+ Anchor-free：无需预设边界框，直接输出框的位置和大小
+ Anchor-based：预设框作为候选框，基于这些框做进一步分类和位置回归。一般更加稳定且易用，如：Faster R-CNN、RetinaNet

#### 1.2 One-stage vs Two-stage:
+ One-stage：一次性预测边界框的位置和类别，常见有YOLO，SDD
+ Two-stage：两个阶段，先生成候选框，然后对框内物体类别判断，对位置回归，如Faster-RCNN

### 2. Anchor和Proposal
Anchor 是一些预定义的矩形框
Proposal是指可能包含物体的矩形框/候选框

anchor是proposal生成的基础，通过预拟合框识别不同尺度的物体，再提取出候选框。

目标检测一般将两者同一起来。会利用生成的anchors对各个位置特征图扫描，得到一系列候选框，再进行分类和回归生成最终结果。

### 3. anchor确定策略
+ 自定义anchor：根据实际应用场景手动定义anchor的大小和比例，用于小目标检测
+ 生成anchor：根据训练数据集的特征进行自适应生成，用于大目标检测

##### anchor的缺点
+ 尺寸需要仔细设计
+ 不具备通用性
+ 负样本过多（focal loss？）
+ 计算IOU耗时

##### 多尺度预测
选取不同尺度的特征图进行box生成，称为多尺度预测。对于较为浅层的卷积，感受野小，适合表示小物体；深层的卷积核，感受野大，可以表示更大的物体。通过不同尺度的组合，可以表达范围更广的物体。

##### 如何利用神经网络生成box：

对于某张卷积出来的特征图其通常有着某一个深度，也就是通道数，我们就是利用这个维度来表示box。

例如：某次卷积之后得到的特征图为19×19×(3×4)，其中19×19是特征图尺寸，可以认为特征图上每一个“像素”都代表一张图片上的位置，而深度3×4则表示对这个位置有三个box，每个box有四个元素(x_min, y_min, x_max, y_max)。对于每个特征图上的像素生成的anchor框大小核数量都是一样的。

### 后处理



### 损失函数
+ 类别损失
+ 定位损失
预测了N个元素，真实值y只有一个，用空来填充到N大小，然后需要找到一个N个元素的排序$\sigma\in\Sigma_N$满足下列条件：
$$
\hat{\sigma} = \arg\min_{\sigma\in\Sigma_N} \sum_i^N\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)})
$$
其中$y_i=(c_i, b_i)$c_i是类别标签，b_i是定义了真实box中心坐标和宽高的4维数组。
对于下标$\sigma(i)$的元素，我们定义类别概率为$\hat{p}_{\sigma(i)}(c_i)$，预测的box为$\hat{b}_{\sigma(i)}$，

$$
\mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)}) =-\mathbb{1}_{\{c_i\neq\varnothing\}}\hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\sigma}(i))
$$
核心含义就是使得最接近的预测框和真实值匹配，其它往后排。然后定义匈牙利损失损失：
$$
\mathcal{L}_{Hungarian}(y_i, \hat{y}_{\sigma(i)}) =\sum_{i=1}^N-\hat{p}_{\sigma(i)}(c_i)+\mathbb{1}_{\{c_i\neq\varnothing\}}\mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}}(i))
$$
另外会将空集对应的类别结果概率缩放10倍来保证数据集平衡

##### 定位损失
一般的l1损失会出现不同box的尺度问题，尽管相近，但由于尺度不太计算结果也会有很大差异。
DETR使用了l1损失和正则化的IoU损失线性组合。IoU是尺度不变性的特征。最后定义如下
$$
\mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{iou}\mathcal{L}_{iou}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1}\Vert b_i - \hat{b}_{\sigma(i)}\Vert_1
$$
这两项会根据样本数量来进行正则化。