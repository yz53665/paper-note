#### 任务描述
使用静态场景下的多个视角图片，合成出任意视角的新图片

### 核心思路

##### 1. MLP学习场景的3D模型表达（映射关系）
$$
F_{\theta}(x, d) -> (c,\sigma)
$$
这里的x是三维坐标，d是相机角度；c是该点发射的颜色rgb，$\sigma$是该点的密度

##### 2. 基于体渲染方程将3D场景渲染成图片

**从3D到图片，这说明NeRF有能力构建3维场景，为下一步Occupancy Network出现提供了基础**

体渲染方程描述了图片上点的成像过程。图片中一个点$P(u,v, 1)$的像素rgb值可以由三维空间中某个点P发出的射线上所有点的c,$\sigma$进行积分得到。

$$
C(r) = \int_{t_n}^{t_f}T(t)\sigma(r(t))c(r(t),d)dt,\ where T(t) = \exp(-\int^t_{t_n}\sigma(r(s))ds)
$$
$$
r(t) = o+td
$$
o是P的位置，d是射线方向，t是从o出发的t时刻。r(t)就是该射线上第t时刻的位置。T是路径上某个点的透过率，可以看出点的密度越大，透过率越小。

##### 3. 模型训练
用SFM算法获取所有图片的位姿，然后每个像素点作为一个样本进行训练。具体来说
+ 用位姿解算出世界坐标
+ 在该射线上采样n个点，用MLP预测n个点的$(\sigma, c)$，使用体渲染方程计算出预测的r'g'b'，和真实值比较得到loss，使用L2距离进行训练。


**基本思想是将成像过程分解，然后用MLP去学习分解元素的映射关系**