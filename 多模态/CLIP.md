《Learning Transferable Visual Models From Natural Language Supervision》
### 核心思想
+ 基于语言描述的监督
+ 对比学习，按照语句整体判断对错，促使模型理解语义 
### 特点
+ 传统计算机视觉任务被训练于预测固定类别的物体，这限制了泛化性和易用性，因为需要额外的标注信息。直接从语言中学习图片中的相关信息是一个更好的替代方案。
+ 具有很强的泛化性，可以泛化到诸如OCR、地理定位、动作识别等。

### 方法
![[Pasted image 20240412195216.png](attach/Pasted%20image%2020240412195216.png)

##### 1. 自然语言监督
监督信息包含在自然语言中。
+ 自然语言不需要严格的语义分类标签
+ 将表达连接到语言上，而不仅仅是学习表达

![[Pasted image 20240414090004.png](attach/Pasted%20image%2020240414090004.png)

##### 2. 选择高效的预训练方法
图像CNN比text Transformer效果要好（蓝线和橙线）
只训练描述整体是否匹配图片，而不是每个字都匹配，用对比学习实现（上图橙线和绿线对比）
+ 生成学习方式可以学习更好的表达，但效率低，所以用对比学习
+ 最大化相同pair的相似度，最小化不同pair的相似度
+ 去掉了表达和多模态空间之间的非线性层，只保留线性映射

#### 3. 使用数据集
已有数据集不符合要求，创建新的数据集
+ 4亿张图片
+ 50万的queries
+ 2万图片么个query

##### 4. 选择和扩展模型（扩展指扩展模型能力）
+ 使用attention pooling，其中的Q是基于全局平均池化所得
+ 图片encoder使用了ResNet-50和ViT做比较
+ 文本使用Transformer，最大长度限制为76，使用[SOS]和[EOS]来标注，EOS会被正则化然后送入多模态嵌入空间
+ 将运算力均匀分配给宽度、深度、分辨率
+ CLIP对于text编码器的结构不敏感

#### 5. 训练
+ 5种ResNet，包括ResNet-50/101，另外还有额外数x倍的运算量，成为ResNet-50x4/x16/x64
+ 3种Vision Transformers
+ 使用网格搜索，在一个模型ResNet-50上训练1轮寻找最优参数组合
+ miniBatch=32768及混合精度

#### 6. 推理
+ 推理时会将遇到过一次的class缓存下来，并继续用于后续预测
### 分析
+ 可以看作是batchsize数量类别的分类网络
+ 图片encoder用于学习图片表达，而text encoder只是用于根据图片表达赋予权重
+ 只提供单个词，由于存在一词多义，CLIP无法判断含义
+ 许多前人工作将类别映射为id，但不提供将id映射回英文单词的关系文件
+ 对于这些类别，本文提出用prompt模板如”A photo of a {label}.“相比单个单词的标签可以更好帮助预测