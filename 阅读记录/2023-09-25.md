---
Title: "Visual Exposes You: Pedestrian Trajectory Prediction Meets Visual Intention"
Year: "2023"
Publisher: IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS 8.5
tags:
  - view
  - image
---
### 轨迹预测的难点和必要性

+ 复杂的关系和理解各种各样的社会行为

### 轨迹预测前人研究问题
只考虑2D坐标带来两个问题：
+ 忽视行人主观性，包括突然转向和不稳定运动
+ 由场景中未标注的行人造成的潜在的碰撞可能性

LSTM不足以处理运动学属性，而最近出现的基于Transformer的方法可以探索数据中的隐藏相关属性。

![[Pasted image 20230925162218.png](../img/Pasted%20image%2020230925162218.png)
1. 如图所示，红色区域未标记的行人导致A行人进行了变道。

而最近只有很少的工作使用了视觉信息吗，并且这些工作忽略了细粒度的行人信息。

### 提出解决思路
引入视觉信息捕捉行人轨迹

### 模型
![[Pasted image 20230925164342.png](../img/Pasted%20image%2020230925164342.png)
#### 预处理
将长度不足的行人轨迹长度补足到12帧，不足的部分坐标和第一次出现坐标一致。

#### 时空Transformer
基于STAR的Transformer模块。

#### 方向模型
分别计算整体方向和视线方向[^1]。

#### 碰撞感知模型
提出概念，视觉意图领域（visual intention field）
![[Pasted image 20230925170440.png](../img/Pasted%20image%2020230925170440.png)、
基于此概念本文提出两条约束：
+ 行人需要在意图领域中
+ 只有对向走的行人才被考虑

防止轨迹碰撞的约束损失函数可以如下表达：
$$
\mathcal{L}_{col} = \sum_{i=1}^{n} | (x_i^t, y_i^t) - (\hat{cx}_s^t, \hat{cy}_s^t) | ^{\frac{1}{2}}
$$

另外为了避免在拥挤场景下行人框过度重叠，使用了损失函数进行约束：
$$
\mathcal{L}_{IoU} = \sum_{(i,j)}^{} \frac{Intersection(B_i^t, B_j^t)}{Union(B_i^t, B_j^t)}
$$
表示交集占并集的比例。
最终该模块的整体损失函数如下：
$$
\mathcal{L_{CP}} = \mathcal{L}_{Col} + \lambda \mathcal{L}_{IoU}
$$
#### 急转弯优化模型
本文统计了各类碰撞、急转弯样本的比例，但并未给出标准。
![[Pasted image 20230925214208.png](../img/Pasted%20image%2020230925214208.png)
而如果行人的视线角度与行走角度出现较大偏差，则行人有更多可能会朝着intention方向行走，所以本文又提出了基于余弦角度的偏离损失函数，使得模型将更多注意力放在预测行人的intention上面：
$$
\mathcal{L_{Dev}} = Soft(\sum_{i,p=1}^{n} \frac{D_p^t\cdot D_i^t}{| D_p^t | \cdot |D_i^t|})
$$
#### 损失函数
除了上文提到的两个损失函数外，还有一个预测坐标的损失函数：
$$
\mathcal{L}_e = \frac{1}{n} \sum_{i=1}^{n} \parallel \hat{Y}_i - Y_i \parallel _2^2
$$
总体损失函数：
$$
\mathcal{L} = \mathcal{L}_e + \alpha \mathcal{L}_{CP} + \beta \mathcal{L}_{Dev}
$$

### 实验验证

#### 消融实验
损失函数参数实验：
![[Pasted image 20230925221158.png](../img/Pasted%20image%2020230925221158.png)
#### 复杂度分析及各模块功能分析
![[Pasted image 20230925221301.png](../img/Pasted%20image%2020230925221301.png)

### 总结与讨论

该文工作量较大，存在较多对数据的处理。创新性一般。
#### 模型模块
使用了额外的模型识别图中行人
另外使用时空Transformer+FC来提取特征和预测轨迹
#### 关键词
本文只是加入了一个transformer模块就在关键词中加了transformer。

#### 其它想法
思考：每个行人都独立考虑是否过于复杂？可以考虑对场景中的行人进行多轨迹统一预测。而当前前人多轨迹统一预测方法存在诸多不足，总是考虑同时预测场景中所有的人，这样造成运算量过大，且许多无关的行人被考虑进来。而文章[[2023-09-23.md]]通过基于距离的启发式方法来筛选相关的个体。但这会造成intention被忽视，实际的行人并不会在快要撞上时才考虑相关行人。更多是基于个人的判断，如对方的前进方向，速度等因素，总结为行人对观察对象未来轨迹预测问题，而行人自己则是需要结合自己的intention，在观察到的这些周围人的未来轨迹中找到既可以保持社交距离、又是直线最短的轨迹。

另外行人前进时的intention不应该是一个点，而是一个大概的方向，可以使用多个分布点来表达（stochastic的由来）

当前基于视角的预测存在不足，如视角的判定是以行人前进方向来确定的，这与现实中行人左右观察的习惯不符。

[^1]: Detecting attended visual targets in video. CVPR2020