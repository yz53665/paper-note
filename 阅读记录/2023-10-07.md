---
Title: "MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction"
Year: "2020"
Publisher: CVPR
tags:
  - memory
Datasets:
  - KITTI
  - Oxford RobotCar
  - Cityscapes
---
# 轨迹预测的难点和必要性


# 记忆网络前人研究问题
RNN、LSTM、GRU中记忆作为一个单独的隐藏向量将所有时间信息编码。缺乏编码知识的独立单元，也缺乏对长时间轨迹的建模能力。

最近的记忆网络（Memory Augmented Neural Networks）使用一个控制网络和一个外置的记忆区。目前已应用于小样本学习、问答任务、在线学习
# 模型
![[Pasted image 20231007170303.png](../img/Pasted%20image%2020231007170303.png)
### 相似度计算
采用余弦相似度进行计算：
$$
s_i = \frac{\pi^k\pi^i}{\parallel \pi^k \parallel  \parallel \pi^i \parallel  }, i\, =\ 0,\cdots,|M|
$$
### 特征提取
![[Pasted image 20231007171309.png](../img/Pasted%20image%2020231007171309.png)
通过重构轨迹来训练编解码器，其中重构包括了过去的轨迹，好处：
+ 可以同时训练过去和未来轨迹编码器
+ 保证预测轨迹和观察轨迹相匹配
+ 使得模型可以生成轨迹，而不是仅仅是记忆的复制品

### 记忆内容控制
一般需要训练控制器在获取到样本时给出加入样本库的概率$P(\omega)$，对于本文来说，是否加入记忆库不光与当前记忆状态有关，还与记忆库状态有关。所以将特征提取时的重构误差也加入损失函数构建：
$$
\mathcal{L}_c = e \cdot(1-P(\omega)) + (1-e) \cdot P(\omega)
$$
当误差e小的时候，说明当前记忆充分，不需要这个样本，最小化概率；当e大的时候，说明模型缺乏当前样本的记忆，最大化概率。
当然这有一个前提是需要限制e在[0,1]内，于是本文提出了一种基于阈值的误差计算方式：
$$
e = 1 - \frac{1}{N} \sum_{i=1}^{N}\mathbb{1}_i(\hat{x}_F, x_F)
$$
当$\hat{x}_F$偏离真实值超过阈值$th_{4s} = 2m$时，这里的$\mathbb{1}$取1，否则为0。这里的m表示时间步，也就是说离出发点越远，允许的误差越大。

### 数据预处理
这里将轨迹进行正则化（将起始点移动到坐标原点，并将轨迹与y轴正切）来获得平移旋转不变性。

# 实验验证

### 在线学习能力测试
![[Pasted image 20231007191516.png](../img/Pasted%20image%2020231007191516.png)

最终记忆库包含了约16%的样本。

### 消融实验
![[Pasted image 20231007193202.png](../img/Pasted%20image%2020231007193202.png)
这里面比较有意思的是第4行，不使用轨迹正则化，也就是轨迹不具有平移旋转不变性，可以看到获取的记忆数量明显更多了。

### 存储记忆可视化
![[Pasted image 20231007193533.png](../img/Pasted%20image%2020231007193533.png)
图中的点是编码后再用t-SNE降维得到的，所有解码后对应的轨迹也绘制在其中。所有轨迹都有向上的趋势，这是由于文章中使用的旋转平移不变性。而相似的轨迹都被聚类到一起，说明了编解码结构的有效性。
并且从中可以看到提取的特征基本是依靠速度和方向来分组。

### 解码器分析
![[Pasted image 20231007195151.png](../img/Pasted%20image%2020231007195151.png)

使用不同的过去轨迹，但使用相同的未来记忆，对比重构的未来结果。都符合了记忆轨迹的相同趋势，但由于历史轨迹的不同，做出了不一样的选择，过去走的更慢，重构轨迹更往下偏；反之往上偏。
还有改进空间，不同行走速度(相同前进方向)不应该改变方向。

# 总结与讨论
