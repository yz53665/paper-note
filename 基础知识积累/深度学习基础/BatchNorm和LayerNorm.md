为什么BatchNorm和LayerNorm有用？
1. 减少内部协方差偏移。避免每一层偏移0，导致学习困难。进行归一化可以稳定输入
2. 稳定化梯度。保持每层输出均值和方差稳定。减少梯度消失和爆炸
3. 减少网络对数据分布的依赖，降低过拟合风险，提升模型泛化性能

BatchNorm和LayerNorm的区别？
1. BatchNorm在CNN中应用较多，这是由于图片内部信息相对稀疏，需要考虑整个batch才能得到相对的类别关系
2. LayerNorm在Transformer中应用更多，这是由于文本token之间的信息更为密集，需要提取词句之间的相互关系，并且还有padding的影响
