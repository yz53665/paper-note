##### 功能
计算预测值和真实值之间的差异值（损失值），通过损失值，模型反向传播来更新各个参数，最终目的是降低真实值和预测值之间的损失，达到学习的目的。


## 1. 基于距离度量的损失函数

将输入数据映射到基于距离度量的特征空间上，然后采用合适的函数度量样本真实值和预测值之间的距离，距离越小性能越好。

### 1.1 MAE损失函数
$$
L(Y|f(x)) = \frac{1}{n}\sum_{i=1}^N | Y_i - f(x_i)|
$$
![[Pasted image 20240313213749.png](../attach/Pasted%20image%2020240313213749.png)
##### 优点
+ 不论对什么值都有稳定的梯度
+ 不会导致梯度爆炸
+ 具有稳健的解

##### 缺点
+ 在中心是折点，不能求导
+ 由于梯度稳定，会在解的附近发生持续震荡

### 1.2 均方误差损失函数（MSE）
$$
L(Y|f(x)) = \frac{1}{n}\sum_{i=1}^N (Y_i - f(x_i))^2
$$
![[Pasted image 20240313213939.png](../attach/Pasted%20image%2020240313213939.png)
常用于回归问题中。
##### 优点：
+ 无参数
+ 计算成本低
+ 各点光滑连续，方便求导

##### 缺点
+ 离梯度远，值很大，可能导致梯度爆炸
+ 接近真实值变得平缓，训练变慢

### 1.2 L1 & L2 损失函数
L1范数损失函数，或称最小绝对值偏差（LAD），最小绝对值误差（LAE）
$$
L(Y|f(x)) = \sum_{i=1}^N |Y_i - f(x_i)|
$$
L2损失函数，也称为最小平方误差（LSE）
$$
L(Y|f(x)) = \sum_{i=1}^N (Y_i - f(x_i))^2
$$
又被称为欧氏距离。常用于回归问题、模式识别、图像处理。
##### 优缺点：

| L1          | L2         |
| ----------- | ---------- |
| 稳定的梯度       | 不稳定的梯度     |
| 相对鲁棒        | 相对不鲁棒      |
+ 鲁棒性：L1对离群值不敏感，L2的离群值会被异常放大，**模型会被迫过分关注异常值，牺牲了很多正常样本**，因为正常样本误差较小
+ 梯度：L1梯度固定，L2在远离中心点的区域会出现一定问题
+ L1会在最优附近来回震荡，L2则可以缓慢接近最优
![[Pasted image 20240314212746.png](../attach/Pasted%20image%2020240314212746.png)
+ L1正则倾向于选择坐标轴上的特征，大部分无用特征会被置0，有特征选择作用（造成特征稀疏）
+ L2正则会让特征权重不过大，使得权重平均

### 1.3 Smooth L1损失函数
$$
L(Y|f(x)) = 
\begin{cases}
\frac{1}{2}(Y-f(x))^2 & |Y-f(x)| \lt1\\
|Y-f(x)| - \frac{1}{2} & |Y-f(x)| \ge 1
\end{cases}
$$
相比L1修改0点不平滑的问题。

### 1.4 Huber 损失函数
$$
L_{\delta}(Y|f(x)) = 
\begin{cases}
\frac{1}{2}(y - f(x))^2,& |y-f(x)|\le \delta\\
\delta|y-f(x)| - \frac{1}{2}\delta^2, & |y-f(x)|\gt\delta
\end{cases}
$$
超参数$\delta$趋近于0，Huber趋向于MSE；趋近于无穷，Huber趋向于MAE。
![[Pasted image 20240314162217.png](../attach/Pasted%20image%2020240314162217.png)
当误差大于$\delta$，可以近似为MAE；当小于等于，则为MSE。
##### 优点
+ 具有连续导数
+ 避免震荡
+ 对异常点有更好的鲁棒性
##### 缺点
+ 引入了额外的参数
## 2. 基于概率度量的损失函数
将样本间相似性转换为随机事件出现的可能性，即通过度量样本的真实分布与其估计的分布之间的距离，判断两者相似度，常用于分类问题
### 2.1 KL散度/相对熵

$$
D_{KL}(p\parallel q) = \sum_{i=1}^N p(x_i)\log\frac{p(x_i)}{q(x_i)}
$$
这里的$p$代表真实分布，$q$代表预测分布。两个分布相似度越高，KL散度越小。这是一种非对称的度量方法。
##### 缺点
+  非对称度量，JS散度优化了这一点
### 2.2 交叉熵损失/对数损失函数
多分类下：
$$
L(Y|f(x)) = -\sum_{i=1}^NY_i\log f(x_i)
$$
二分类（y为0、1）：
$$
L(Y|f(x)) = -\log f(x_i)
$$
刻画了实际输出与起往输出概率之间的相似度。值越小，两个概率分布越接近。常在正负样本不均衡的问题中使用。
##### 优点
+ 有效避免梯度消散

当样本不均衡时，常在每类前面加一个参数$\alpha$权重。
### 交叉熵 熵和KL散度关系
KL散度=交叉熵 - 熵
交叉熵大于熵,即KL散度为两个熵的差异
### 2.3 Focal loss
主要为了解决难易样本不均衡的问题。可以分为四个类别：

|     | 难   | 易   |
| --- | --- | --- |
| 正   | 正难  | 正易  |
| 负   | 负难  | 负易  |
主要思想是通过一个动态缩放的因子，再训练过程中动态降低易区分样本的权重，将重心转移至难区分的样本。
进化路径：cross Entropy-> Balanced Cross Entropy -> Focal Loss
Balanced Cross Entropy引入权重因子$\alpha$，负样本权重就是$1-\alpha$:
$$
L(Y|f(x)) = -\alpha\log f(x_i)
$$
BCE解决了正负样本不平衡的问题，但没有区分简单还是易分样本，当易分样本多时会造成训练过程忽略难分样本。
模型要重点关注难分的样本，因此需要将置信度高（确定程度高，容易分）的损失降低一点：
$$
FE = -(1-p)^\gamma \log(p)
$$
$\gamma\in[0,5]$，当取0时就是一开始的CE。当p趋近于1时，说明是易分样本，所以调制因子趋向于0。
+ 调制因子用来控制易分样本的损失贡献，越大易分样本越小
+ $\alpha$用于调节正负样本的损失比例
### 2.4 Swish Loss
$$
Swish(x) = x Sigmoid(\beta x)
$$
![[Pasted image 20240820190113.png](../../img/Pasted%20image%2020240820190113.png)
优点：
+ 在0附近能提供比ReLU更加平滑的结果，所以效果更好



## 3. 针对部分不可导的点处理

### 3.1 次梯度（subgradient）
对于定义的凸函数及其取值空间U，一个欧式空间下向量$v$可以在点$x_0\in U$处被称为次梯度，在满足对任意一点$x\in U$都有：
$$
f(x) - f(x_0)\ge v\cdot(x-x_0)
$$
次梯度的集合永不为空。

### 3.2 概率小
由于数据维度特别大，而损失函数中仅有可数的不可导点，那么可以使得反向传播恰好在该点求导的概率可以近似认为是0。如100维的数据，恰好100个都是0的概率非常小。

pytorch 中会直接将其置0。