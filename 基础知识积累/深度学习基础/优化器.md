## SGD（随机梯度下降）
$$
\omega_{t+1} = \omega_{t} - \alpha\cdot g_t
$$
直接用当前时刻的梯度，计算偏差量 
##### 优点
+ 每次只用一个样本更新参数，训练速度快
+ 波动大，有利于在局部极小值点间来回跳跃
##### 缺点
+ 遇到局部最优或鞍点时，梯度为0，无法更新
+ 沿陡峭方向震荡，平缓维度进展缓慢，难以迅速收敛

## Momentum
在SGD基础上加了一个一阶动量，引入惯性概念：
$$
m_t = \beta \cdot m_{t-1} + g(w_{t})
$$
$$
w_{t+1} =w_t-\alpha\cdot m_t
$$
考虑到一阶矩估计，做一个累计，原始论文里面是说要考虑到梯度速度的影响。
+ 缓解了SGD在局部最优点梯度为0的问题
+ 如果沟壑比较深，还是会被困在局部最优

## Nesterov Accelerated Gradient（NAG）
moemntum的公式中每次更新会有一个恒定值$\alpha\beta m_{t-1}$，NGA认为，既然必定要走，不如让模型参数每次直接多更新这一部分再计算参数：
$$
m_t = \beta \cdot m_{t-1} +  g(w_{t} - \alpha\beta m_{t-1})
$$
$$
w_{t+1} =w_t-\alpha\cdot m_t
$$
其中式子1等效于：
$$
m_t = \beta \cdot m_{t-1} +  g(w_{t}) + \beta [g(w_{t})-g(w_{t-1})]
$$
本质上相当于多加了一个二阶导信息，也就是加入了变化率。如果导数增大的速度在加快，那就可以更快一点，如果增大速度再减慢，那就会产生额外阻力。

## AdaGrad（自适应学习率算法）
SGD及其变种以同样的速率更新每个参数，大量经验认为：经常更新的参数累积了大量学习到的知识，不希望被单个样本影响太大，学习速率需要慢一点；更新少的参数，了解信息太少，希望学习速率大一点。
即，为了防止方差过大，需要进行缩放。
**需要考虑如何度量历史更新频率**
可以使用二阶动量和：
$$
V_t = \sum_{\tau=1}^t g_\tau^2
$$
梯度计算：
$$
\eta_t = \alpha \cdot \frac{m_t}{\sqrt{V_t}}
$$
$$
w_{t+1} = w_t - \eta_t
$$
相当于学习率由$\alpha$变成了$\frac{\alpha}{\sqrt{V_t}}$，当参数更新越频繁，二阶动量就会越大，学习率就越小。
##### 优点
+ 稀疏数据场景下表现非常好
+ 此前工作主要关注优化梯度前进方向上，AdaGrad首次使用二阶导来控制学习率
##### 缺点
+ 二阶动量和是单调递增的，会使得学习率不断缩小至接近0，使得训练提前结束，忽略掉后续的数据中的知识

## AdaDelta/RMSProp
考虑不那么激进的策略，即不累计全部历史梯度，只关注过去一段时间窗口的下降梯度
$$
V_t = \beta V_t + (1-\beta)g_t^2
$$

## Adam
将RMSProp和Momentum结合起来就是Adam：
$$
m_t = \beta_1\cdot m_{t-1} + (1- \beta_1)g_t
$$
$$
V_t = \beta_2\cdot V_{t-1} + (1-\beta_2)g_t^2
$$
$$
w_{t+1} = w_t -\alpha\cdot \frac{m_t}{\sqrt{V_t}}
$$
##### 优点
+ 一阶动量控制梯度方向，二阶动量控制学习率步长

##### 缺点
+ 可能不收敛：二阶动量是窗口累积，随着窗口变化，遇到的数据可能巨变，再训练后期可能出现学习率震荡，导致模型无法收敛。可以对二阶动量进行控制，保证V_{t-1}始终小于V_t
$$
	V_t = \max(\beta_2\cdot V_{t-1}+(1-\beta_2)\cdot g_t^2, V_{t-1})
$$
+ 可能错过全局最优解：会对前期特征过拟合，后期由于学习率小无法纠正，影响收敛
Adam的优点主要在于：
- 考虑历史步中的梯度更新信息，能够降低梯度更新噪声。
- 此外经过偏差校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。

但是Adam也有其自身问题：可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。二者似乎都没法很好避免局部最优问题。

一阶矩估计可以更好的帮助算法适应目标函数梯度的整体趋势。二阶矩估计则可以用于捕捉梯度的方差或波动性，如果目标函数表面上存在平坦区域或高曲率区域时，则此时梯度的方差可能变化很大，将其作为分母便可以减小学习率以提高稳定性；相反则会增加算法的学习率以提高收敛速度

## AdamW
Adam泛化性不好。泛化性和权重衰减有关系，一般的L2正则化项会直接加在损失函数上。但在反向传播时，如在Adam中，L2正则化项原有参数上再进行衰减或加权平均，这会大大削弱L2正则化项的作用：
![[Pasted image 20240417160725.png](attach/Pasted%20image%2020240417160725.png)
AdamW则将该权重衰减项单独提取出来加到参数上，避免模型泛化性受到影响。

# 选择的tricks
+ 模型如果非常稀疏，考虑自适应学习率算法
+ 可以线用Adam快速实验优化，然后精调SGD进行极致优化
+ 可以线用小数据集实验
+ 考虑不同算法组合。Adam快速下降，然后用SGD调优
+ 数据集一定要shuffle，可以避免特征集中出现
+ 制定合适的学习率衰减策略