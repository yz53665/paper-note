##### 为什么要使用激活函数
每一层输入都是线性求和，而存粹线性组合无法解决更加复杂的问题。激活函数可以引入非线性，使得网络可以逼近更多非线性模型

##### 激活函数通常的性质
+ 连续可导
+ 导数简单
+ 导函数值域要在合适区间，否则影响效率和稳定性

# 常见激活函数

## 1. Sigmoid (Logistic)
+ 范围：(0, 1)
$$
f(x) = \frac{1}{1+e^{-x}}
$$
![[Pasted image 20240303201529.png](../attach/Pasted%20image%2020240303201529.png)
#### 使用场景
+ 输出范围在0到1
+ 输出概率
+ 梯度平滑，避免跳跃输出值
+ 函数可微
+ 明确预测（非常接近0或1）
#### 不足
+ 梯度消失（网络中大量包含sigmoid很容易出现饱和状态）
+ 输出值不以0为中心，使得后一层发生偏移，收敛速度变慢
+ exp计算成本高昂
## 2. Tanh/双曲正切
+ 范围：(-1, 1)
$$
f(x) = \tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} = \frac{2}{1+e^{-2x}}-1
$$
与sigmoid关系：
$$
\tanh(x) = 2sigmoid(2x)-1
$$
![[Pasted image 20240303202633.png](../attach/Pasted%20image%2020240303202633.png)
使用场景优先于sigmoid
#### 使用场景
+ 以0为中心
#### 不足
+ 与sigmoid类似，存在梯度消失
一般二分类问题tanh作为隐藏层，sigmoid作为输出层

## 3. ReLU/线性修正单元（Rectified Linear Unit）
$$
f(x) = max(0, x)
$$
![[Pasted image 20240303202652.png](../attach/Pasted%20image%2020240303202652.png)
#### 优点
+ 输入为正，导数为1，改善梯度消失
+ 计算快
+ 生理学合理性。（单侧抑制、兴奋程度可以很高）

#### 不足
+ Dead ReLU。输入为负完全失效
	+ 训练过程中某次更新可能导致某个神经元永久死亡，无法再次更新
+ 不以0为中心

导数在0处左右不相等，所以不可导。
实际操作中使用次梯度，即0~1之间:
$$
c \le \frac{f(x) - f(x_0)}{x-x_0}
$$
工程上为了方便常常取0.

## 4. Leaky ReLU
让负数取值很小，这里的$\gamma$为取值很小的数
$$
f(x) = max(0, x) + \gamma min(0, x)
$$
![[Pasted image 20240303203056.png](../attach/Pasted%20image%2020240303203056.png)
#### 优点
　+ 缓解Dead ReLU
　+ 范围更大，在负无穷到正无穷
#### 不足
+ 实际应用中不能完全证明优于ReLU

## 5. Softmax
用于K分类问题，第i个输出
$$
S_i = \frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}}
$$
一般与交叉熵结合使用
#### 优点
+ 各元素输出和为1
+ 输出之间彼此相关
#### 不足
+ 0点不可微
+ 负输入梯度为0，产生永远不激活的死亡神经元
+ 指数运算运算量大
+ 除法运算
+ 对于异常值比较敏感
