![[Pasted image 20240314175158.png](../attach/Pasted%20image%2020240314175158.png)
### 递归神经网络——RNN
目的：处理时间序列问题
特点：将该时刻信息传递给下一个时刻，在短时间序列数据预测效果好
存在问题：
+ 长期依赖问题。信息很难以远距离传递。
+ 对于任意信息递归足够多次都会导致（或者训练时间过长、或者参数过度调节）：
	+ 训练时梯度消失问题
	+ 梯度爆炸问题
![[Pasted image 20240302200659.png](../attach/Pasted%20image%2020240302200659.png)
### 长短期记忆网络——LSTM
目的：解决长期依赖问题，RNN中的梯度消失/爆炸问题
特点：红色圆圈为按位操作，黄色方框为网络（有权重和偏置）
![[Pasted image 20240302200705.png](../attach/Pasted%20image%2020240302200705.png)

分块结构：
总体的历史信息：
![[Pasted image 20240302201006.png](../attach/Pasted%20image%2020240302201006.png)
遗忘门：将上一时刻部分信息遗忘。整合后的信息会通过sigmoid层将信息压缩到（0，1）内，对于需要遗忘的位取0，完全记忆的位取1，再与历史信息相乘。
![[Pasted image 20240302201016.png](../attach/Pasted%20image%2020240302201016.png)
$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

输入门：右侧tanh用于激活，左侧sigmoid用于控制有多少信息加入到总体信息流中
![[Pasted image 20240302201645.png](../attach/Pasted%20image%2020240302201645.png)
$$
G_t = tanh(W_c\cdot[h_{t-1}, x_t]+b_c)
$$
$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t]+b_i)
$$
输出门：
$$
O_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$
当前状态：
$$
C_t = C_{t-1}\odot f_t + G_t\odot i_t
$$
$$
h_t = O_t \odot \tanh(C_t)
$$
##### 为何使用Tanh作为激活函数？
+ 防止梯度消失问题，需要一个二次导数在大范围内不为零的函数
+ 为了便于凸优化，需要一个单调函数
+ tanh函数一般收敛更快
+ tanh函数求导占用系统资源更少

### 门控递归单元——GRU
特点：有重置门r和更新门z
$$
r = \sigma(W_r\cdot[h_{t-1}, x_t])
$$
$$
z = \sigma(W_z\cdot[h_{t-1}, x_t])
$$
重置门：用于控制上一个状态的输入量大小（0~1之间）：
$$
h'= \tanh(W\cdot[h_{t-1}\cdot r, x_t])
$$
更新记忆：同时进行遗忘和记忆
$$
h_t = (1-z)\odot h_{t-1}+z\odot h'
$$
使用z作为权重来同时控制记忆和遗忘的比例。

对比LSTM：
+ 少了一个门，参数量和运算量更小
+ 将h和C合并表达，减少需要存储的信息

