TIPS：所有问题均可以从 数据、模型、训练、损失函数、推理几个维度来考虑入手解决。

## Batch Size 的影响
太小
+ 耗时长，效率低
+ 难以收敛
##### 增大的优缺点
 优点
+ 减少训练时间
+ 梯度计算更加稳定
缺点
+ 所需内存容量增加
+ 泛化能力下降
+ 收敛速度可能会下降（需要综合考虑所有样本，互相抵消后最终下降的梯度就很小）

##### 如何平衡
+ batchsize 增加，增加epoch
+ 增大到某个阈值，达到收敛精度最优

## 学习率

太大
+ 无法收敛

太小
+ 收敛很慢
+ 无法跳出局部最优
## 过拟合及欠拟合
过拟合可能原因：
+ 数据量
+ 训练集和验证集分布不一致
+ 模型复杂度
+ 数据质量差
+ 过度训练

#### 欠拟合如何处理？

数据：增加训练数据，做好数据增强
损失函数：减小正则化
模型：减小drop out，增加参数量，使用预训练
训练：是否学习率太高？增加训练轮次。

#### 过拟合如何处理？

数据：数据增强、增加数据，保证数据集分布一致性
损失函数：增加L1、L2正则化，
模型：加入drop out，BatchNorm，权值共享
训练：early stop，adamw

#### dropout在pytorch中是怎么实现的
以p的概率将输入tensor的某些位置置0，而其它值除以（1-p）来进行放大。
dropout 对于参数多的场景下可以促使模型学习不同的表达，或者学习多个备份，相当于多个分类器集成；对于参数少的场景，也可以为模型提供输入噪音。
##### BatchNorm怎么处理的
+ 计算/更新均值和方差
+ 使用均值方差将每个元素标准化
+ 两个可学习参数$\gamma,\beta$用于恢复原始网络要学习的特征分布
![[Pasted image 20240324100641.png](../attach/Pasted%20image%2020240324100641.png)
![[Pasted image 20240324100650.png](../attach/Pasted%20image%2020240324100650.png)


## 梯度消失和梯度爆炸
梯度饱和：梯度越来越趋近于一条直线，变化很小
梯度爆炸：梯度呈现指数级增长
梯度消失：呈现指数级别递减
##### 1. 梯度消失
+ ReLU及其变体
+ 残差结构
+ BatchNorm
+ Xavier初始化
	+ 使得每一层输出方差等于输入方差
##### 2. 梯度爆炸
+ 梯度裁剪
	+ 绝对值裁剪，使其梯度中每个元素不超过某个阈值
	+ 范数裁剪，根据梯度范数等比例缩小
+ L1L2正则化（限制W）
+ Xavier初始化

### 样本不均衡问题：

数据：直接扩大少量样本数据集，过采样、欠采样、伪标签 
损失函数：调整少数类别的权重，focalloss （针对难易样本，不是正负样本）
Inference：调整预测的阈值。 
模型：采用大规模预训练，只是在本数据集上做一个微调。 
训练：随机剔除正样本。

#### bagging和boosting

Bagging：
+ 有放回抽样n轮，训练n个分类器再集成起来。
+ 主要降低方差，对偏置无明显作用
+ 随机森林
Boosting：
+ 每一轮的训练集不变，提升前一个分类器错分样本的权重，然后通过线性组合来组合弱分类器。
+ 序贯式集成
+ 降低偏置（bias）
+ AdaBoost、GBDT、XGBoost

## 生成式模型(generative approach)和判别式模型(discriminative approach)：

**决策函数Y=f(x)**：输入一个x，它就输出一个y值，这个y与一个阈值比较，根据比较结果判定x属于哪个类别。

**条件概率分布P(y|x)**：输入一个x，它通过比较它属于所有类的概率，然后预测时应用最大后验概率法（MAP）即比较条件概率最大的类为x对应的类别。

##### 判别式模型
学习模型本身。具体来说，学习决策函数f(x)或者是条件概率分布$P(y|x)$作为预测模型。

例如，线性回归、对数回归、线性判别分析、支持向量机、 boosting、条件随机场、神经网络等
##### 生成式模型
学习联合概率密度分布$P(x,y)$，然后生成条件概率分布$P(y|x)$作为预测模型
也就是估计贝叶斯公式中的似然P(x|y)和先验概率P(y)：
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$
#### 一个通俗的例子
假如你的任务是识别一个语音属于哪种语言，对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的：

（1）学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你说，你就可以知道他说的是什么语音.

（2）不去学习每一种语言，你只学习这些语言之间的差别，然后再判断（分类）。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。

第一种方法就是生成方法，第二种方法是判别方法。