[用人话讲明白逻辑回归Logistic regression - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/139122386)

输出经过sigmoid函数的多元线性方程：
![[Pasted image 20240907001225.png](image/Pasted%20image%2020240907001225.png)


逻辑回归不能使用平方损失优化，函数不为凸函数，无法求解全局最优，所以用用极大似然估计求解：

似然性：**似然性(Likelihood)**意思是一个事件实际已经发生了，反推在什么参数条件下，这个事件发生的概率最大。

求导：
![[Pasted image 20240907002200.png](image/Pasted%20image%2020240907002200.png)
记：
![[Pasted image 20240907002321.png](image/Pasted%20image%2020240907002321.png)
![[Pasted image 20240907002344.png](image/Pasted%20image%2020240907002344.png)
最终导数
![[Pasted image 20240907002525.png](image/Pasted%20image%2020240907002525.png)

### 问题

全零初始化会怎么样？
所有位置的参数计算梯度相同，每次梯度下降更新距离也相同，模型退化成单个可拟合参数，拟合能力大幅减弱

没有解析解

SVM和LR差异？

|       | LR          | SVM                                                           |
| ----- | ----------- | ------------------------------------------------------------- |
| loss  | 基于极大似然估计    | 基于集合间隔最大化                                                     |
| 风险    | 经验风险最小化     | 结构风险最小化：1. 训练误差和模型复杂度之间平衡，防止过拟合 ； 2. SVM的loss第一项可以看作L2正则化项 2. |
| 点     | 考虑所有的点      | 只考虑分解平面附近的点                                                   |
| 结果    | 概率          | 不能产生概率，只有正负                                                   |
| 非线性   |             | 核函数                                                           |
| 计算复杂度 | 简单，可以用于大数据集 | 计算复杂，效果更好，适合小数据集                                              |
| 多分类   | one or rest | 一般不用与多分类                                                      |
