# 1. 概念
## 1.1 基本定义
智能体（Agent）在复杂不确定的环境中极大化能获得的奖励。
+ 两个实体：
	+ 智能体（agent）
	+ 环境（environment）
+ 三要素：
	+ 状态（state）
	+ 动作（action）
	+ 奖励（reward）


是除了监督、非监督之外的第三种学习方法

主要特点：
+ 试错学习：通过试错获得最佳策略
+ 延迟回报：事后才给出状态

## 1.2 应用
游戏：
![[Pasted image 20240601155130.png](../../img/Pasted%20image%2020240601155130.png)
机器人：
![[Pasted image 20240601155152.png](../../img/Pasted%20image%2020240601155152.png)

推荐广告：
![[Pasted image 20240601155223.png](../../img/Pasted%20image%2020240601155223.png)
![[Pasted image 20240601155316.png](../../img/Pasted%20image%2020240601155316.png)


## 1.3 相关术语

### 1.3.1 策略：
决定下一步执行什么行动。可以是确定的：
$$
a_t = \mu(s_t)
$$
随机的：
$$
a_t \sim \pi(\cdot|s_t)
$$

### 1.3.2 状态转移
一般认为随机的：
$$
p(s'|s,a) = \mathbb{P}(S'=s'|S=s,A=a)
$$

### 1.3.3 回报
别称cumulated future reward
$$
U_t = R_t+R_{t+1}+R_{t+2} \cdots
$$
未来奖励不如现在的奖励好，所以一般会会加入权重，权重成为折扣率

### 1.3.4 价值函数（Value Function）
使用期望对未来收益进行预测。表达当前策略好坏
![[Pasted image 20240601160200.png](../../img/Pasted%20image%2020240601160200.png)
+ 状态价值函数：当前状态好坏
+ 动作价值函数：未来动作好坏

# 2. 常见强化学习方法与概念
### 2.1 Q-Learning
Q值表维数：状态数x动作数，表示每个状态下执行动作可以获得的收益
可以不断迭代使得Q值表收敛，根据该表可以选择当前状态下最优策略。
![[Pasted image 20240601161918.png](../../img/Pasted%20image%2020240601161918.png)
Q值表：
![[Pasted image 20240601161924.png](../../img/Pasted%20image%2020240601161924.png)
算法流程：
![[Pasted image 20240601171939.png](../../img/Pasted%20image%2020240601171939.png)


## 2.2 Deep Q-Network(DQN)
使用神经网络实现将状态和动作映射到Q值（动作价值函数）

+ 经验回放：学习时利用经验池存储经验取batch更新Q。提高了样本利用率，打乱样本相关性。
+ 固定Q目标：神经网络学习的是固定目标，Q会在一段时间内保持不变

算法流程：
![[Pasted image 20240601172410.png](../../img/Pasted%20image%2020240601172410.png)

存在问题：
+ 小的变化可能导致对应动作选择或不选择，属于不连续函数，无法保证收敛
+ 选择最大Q值在高纬空间是困难的
+ 无法学习随机策略，有些情况下随机策略最优

## 2.3 Proximal Policy Optimization（PPO）
基于策略梯度的强化学习算法，直接根据状态选择动作。省略了DQN估计动作价值的过程。

不通过误差反向传播，而是通过reward直接对行为进行改进。

## 2.4 Trust Region Policy Optimization（TRPO）
基于策略优化的强化学习算法，每次更新是保持策略改变幅度较，确保算法稳定性。