
## 信息量

事件发生概率越低，则发生时带来的信息量越大。并且满足$H(xy)=H(x)+H(y)$，所以信息量公式：
$$
Info(x) = -\log p(x)
$$
## 熵

衡量系统的稳定程度。本质是系统所有变量信息量的期望：
$$
H(X) = -\sum_{x\in X}p(x)\log p(x)
$$
+ 系统越不稳定，熵越高

## 联合熵

多个联合变量的熵：
$$
H(X,Y) = -\sum_{x\in X} \sum_{y\in Y}p(x,y)\log p(x,y)
$$

## 条件熵

在给定条件下的系统熵，本质是条件概率信息量的期望：
$$
\begin{aligned}
H(Y|X) & = \sum_{x\in X}p(x)H(Y|X = x) \\
& = -\sum_{x\in X} p(x) \sum_{y\in Y} p(y|x) \log p(y|x) \\
& = -\sum_{x\in X}\sum_{y\in Y} p(x,y) \log p(y|x)
\end{aligned}
$$

## 相对熵(KL散度)

衡量两个分布对于同一个变量的熵差异情况：
$$
D_{KL}(p||q) = - \sum_{x\in X} p(x) [\log q(x) - \log p(x)] = \sum_{x\in X} p(x) \log \frac{p(x)}{q(x)}
$$

![[Pasted image 20240915224600.png](image/Pasted%20image%2020240915224600.png)

## 交叉熵

和相对熵本质相同，因为我在实际的优化过程中，相对熵的第一项是优化对象，第二项为固定常量，和优化对象成线性关系。为了简化计算，去掉第二项就成了交叉熵

$$
H_{CE}(p,q) = - \sum_{x\in X} p(x)\log q(x)
$$

## JS散度
为了解决KL散度不对称的问题所提出：
$$
D_{JS}(p || q) = \frac{1}{2}[D_{KL}(p||\frac{p+q}{2}) + D_{KL}(q||\frac{p+q}{2})]
$$

## 信息增益

在训练集上用于衡量变量对任务的影响。比如西瓜是否成熟可以计算一个熵，额外变量如瓜蒂、纹理等变量可以辅助判断，减小不确定性，使得系统的熵降低：

$$
g(D, A) = H(D) - H(D|A)
$$
公式上理解，即加入条件A前后的系统熵差值即为条件A带来的信息增益

## 互信息
决定联合分布和分解的边缘分布的相似度

$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
$$
物理含义：
+ 通信前后不确定性减少量。通信前把xy看成是独立的两个子系统熵，通信后的结果属于两者的联合统计结果的两个随机变量，整体的熵为H(X,Y)

和信息增益的差异：
+ 互信息中，两者的地位是相同的；信息增益中，一个是减小另一个变量的手段
![[Pasted image 20240915225836.png](image/Pasted%20image%2020240915225836.png)