
# Flash Attention 1
## 背景

1. 很多模型如GPT3、llama的最大支持长度只有2048，影响或者限制模型长度进一步扩展的关键性因素就是transformer的计算复杂度为O(n^2)，n为序列长度
2. flash attention提出了一种可以加速计算、节省显存的精确注意力机制可以缓解上述问题

## 原始attention的计算复杂度
![[Pasted image 20240821224849.png](../../img/Pasted%20image%2020240821224849.png)
这里的运算量就是4x5x4，也就是$N^2d$的复杂度。

#### 1. self-attention计算复杂度
QKV线性层计算，(b, N, d) (d, d)，合计运算量为$3\cdot 2bNd^2$，这里的2包含加法和乘法。
QK相乘：$2bN^2d$
QKV：$2bN^2d$
attention后的线性映射：$2bNd^2$
结果：$4bN^2d + 8bNd^2$

#### 2. MLP计算复杂度
第一个线性层d->4d，第二个线性层4d->d
合计$16bNd^2$

#### 3. logits计算量
$2bNdV$，V为词表大小

## 空间复杂度

## Flash Attention
从上面的计算可以看到，不论是空间还是时间复杂度，所有的复杂度都和长度N呈二次方增长，这些增长都来源于self-attention。

前人工作问题：
过于关注减少浮点数计算，但现代GPU的计算速度远远超过了显存访问速度，当前的主要瓶颈是显存访问速度

而attention的QK结果和softmax结果都要写入显存，具体如下：
1. QK读取两次访问显存，对QK计算结果S写入一次访问，共3次
2. S读取一次，对S的softmax结果P写入一次，共2次
3. 对PV的读取2次，再写入1次，共3次
求和共8次显存独写操作。

### 总结
自注意力存在问题：
1. 显存占用过多，N^2内存要求
2. 显存读写次数多，减慢了运行时间


# Memory-efficienct Attention
将显存复杂度从N^2降低到线性
![[Pasted image 20240821233241.png](../../img/Pasted%20image%2020240821233241.png)
即不直接完整计算attention，而是按单元来计算

## Flash Attention
通过kernel融合，降低HBM独写次数，避免频繁从HBM中读写数据，具体分三步：
1. 每个kernel将输入数据从HBM加载到SRAM
2. 在SRAM中运算
3. 再从SRAM中移动到HBM
通过将kernel融合，可以将多个三步走融合为一个三步走

记
S = QK
P = softmax(S)
O= PV
### 1. 分块注意力tiling
主要解决kernel融合后超出SRAM内存上限的问题：

+ 将融合kernel切分为SRAM大小，送入SRAM中一次性完成所有计算(矩阵乘法、mask、softmax、dropout、矩阵乘法)再写回HBM中
+ 难点在于计算softmax时归一化因子（safe-softmax）需要完整输入数据才能计算

解决方案：
+ 不保存SP两个矩阵（用于反向传播），而是保留两个统计量m和l，在反向传播时借助统计量快速重新计算SP矩阵
+ 其中m是用于safe-softmax计算的最大值
+ l是softmax的分母

分块计算时，每次只计算一块，然后通过递推公式更新m和l，其中m只需不断比较最大值即可，l需要重新调整其中的m值，只需乘上一个$e^{m_{old}-m_{new}}$即可

# Flash Attention 2

版本1不足——不如其他基本操作高效：
+ 前向传播只到达理论FLOPS最大的30-50%，反向传播仅到达最大吞吐量25-35%
+ 矩阵乘法可以达到80-90%

原因：
+ 在GPU上不同县城块和线程束之间工作划分不够好

改进：
+ 减少非矩阵乘法操作的浮点运算次数
	+ 非矩阵乘法耗时长
		+ A100矩阵乘法理论最大吞吐量312TFLOPs/s，非矩阵乘法只有19.5
+ 除了batch和heads维度，在序列长度维度上并行化前向传播和反响传播。优化长序列小batch
+ 一个计算块内部进一步划分给不同线程块，减少通信和显存读写

#### 减少非矩阵乘法算法

再对第每个计算块计算分母和时，不对第一个计算块调整，只有所有计算块统一算完后再统一调整