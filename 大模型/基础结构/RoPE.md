和相对位置编码相比有更好的外推性

## 大模型外推
指预测和输入长度不一致导致模型泛化性能下降的问题


固定位置编码：可以根据无参数的固定公式推导得到，如原始的Transformer
可学习式位置编码：BERT和GPT采用，根据上下文自适应学习得到
相对位置编码：对相对位置建模

+ RoPE是一种固定位置编码策略
+ 配合内积机制可以达到相对位置编码的效果
+ 核心思想：对Query和Key向量作变换，使得其带有位置信息，最终使得attention无需更改实现对位置信息感应

## 优点
+ 固定式编码，无需额外学习成本

公式如下：
![[Pasted image 20240821004627.png](../../img/Pasted%20image%2020240821004627.png)

这里q表示句子中位置为m，维度为d的Query向量。位置信息由$\theta$和m来表达。$\theta$由各元素位置表达：
$$
\theta_i = \frac{1}{10000^{\frac{2i}{d}}}
$$
![[Pasted image 20240821005031.png](../../img/Pasted%20image%2020240821005031.png)
可以看作是每个元素和相邻元素经过sin和cos加权求和的结果

## 和Transformer的位置编码差异

+ 实现相对位置能力的途径不同。sin-cos位置编码由于三角函数性质本身天然可以表达相对位置，而RoPE本身不行，需要经过内积激活相对位置表达能力（原始的三角函数编码经过内积会丢失位置信息）

本质上：**RoPE是对各个位置的token向量根据自身位置m计算角度做逆时针旋转，在Attention的内积操作中，内积能够感知到旋转之后两个向量之间的夹角，这个夹角就是相对位置信息**