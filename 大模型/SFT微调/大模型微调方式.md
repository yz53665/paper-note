# 有监督式微调（SFT）
优点：
1. 利用预训练的先验知识（包括语义、语法）
2. 加速模型收敛
3. 快速应用于多种下游任务
缺点：
1. 需要大量的标注数据，数据量过少可能导致过拟合
2. 难以调整模型结构
3. 性能受限于预训练数据和下游任务的相关性
4. 可能遗忘先验知识

# 人类反馈的强化式学习
优点：
1. 提高模型性能：通过人类反馈，在复杂场景中快速学习高质量策略
2. 一些强化学习方法奖励信号稀疏，人类反馈可以作为补充使之更有效的学习
3. 提升安全性和可靠性，人类反馈可以帮忙识别修正模型训练过程中产生的错误行为，对齐人类观点
4. 加速训练
5. 增强可解释性
缺点：
1. 高成本
2. 主观性
3. 人类反馈存在噪声问题
4. 可扩展性差，手动反馈跟不上训练需求
5. 依赖于人类专家，进一步提高成本
# AI反馈的强化式学习
是指利用人工智能系统生成的反馈信息来改进强化学习模型的方法。

优点：
1. 自动化程度高，减少人类参与的成本
2. 可扩展性强，可以快速扩展到更大范围的训练任务上
3. 一致性更好，避免人类反馈的主观性和不一致性
4. 速度快
5. 可调整与优化，
缺点：
1. 依赖基础模型
2. 缺乏人类洞察能力，无法捕捉某些细微但重要的行为
3. 复杂性，设计和实现能生成反馈的AI系统本身就是一个复杂的任务
4. 反馈噪声，生成反馈可能由于基座模型性能影响带来噪声
5. 潜在反馈循环，如果AI反馈系统和学习模型存在某种反馈循环，可能导致共振现象，导致训练过程不稳定