
# 常见数据类型精度
1 byte = 8 bits

1 KB = 1,024 bytes

1 MB = 1,024 KB

1 GB = 1,024 MB

注意：TF32占用内存不是32bit，而是19bit

![[Pasted image 20240826114514.png](image/Pasted%20image%2020240826114514.png)
# 混合精度训练

训练流程：
![[Pasted image 20240826114632.png](image/Pasted%20image%2020240826114632.png)

总结如下：
1. FP32存储模型备份，初始化其一阶和二阶动量
2. FP32在新空间中转换为FP16
3. FP16模型精进前向和反向传播，得到FP16的梯度和激活值
4. 用FP16的梯度和FP32的一阶二阶动量更新FP32模型权重
5. 重复上述2-4步骤

则主要内存占用来自：
1. 模型（FP32+FP16）
2. 优化器（一阶、二阶动量FP32）
3. 梯度（FP16）
4. 激活（FP16）

### 问题
1. 为什么不都使用FP16？
	这是为了比秒FP16过窄的数值范围导致溢出和舍入误差。
2. BF16和FP16有何差异？
	BF16的数据范围比FP16更大，但精度更差，更难发生数据溢出的现象。而目前业界实践表明大数据范围更加重要。
3. 为何只对梯度和激活值版精度优化，模型还保留了FP32，这不会导致占用更大吗？
	激活值和batch和seqLength有关，一般显存占用会大于模型占用，对于其进行优化的正向作用也大于模型的备份带来负向作用。使用FP32也可以尽可能保留模型的能力
4. 哪些是静态和动态显存占用？
	静态：优化器、模型参数
	动态：激活值、梯度值
	实际计算中，梯度可以近似于模型的FP16大小，激活则不太好算

### 例子
一个8B的模型，使用AdamW优化器优化，**FP32和FP16混合精度**训练，请问训练时显存占用多少？

模型参数：16（BF16）+32（FP32）=48G
梯度参数：16G
优化器：32（FP32）+32（FP32）=64G
64+16+48=128G

实际占用远大于128G（梯度显存占用）

# KV cache的影响
KV cache通过显存换速度，也会增加显存占用。
其核心思想，Decoder的token生成只和当前的QKV和之前所有的KV有关系，所以只需要将之前的所有KV块缓存下来，只计算当前时刻的QKV即可大大减少运算量。
![[image/v2-bb2bb2396d5506a985e46b6fdb6e571d_b.webp]]
### KV cache占用缓存计算

Memory = Batch_size X Seq_length X embedding_size X num_layers X 2 X 2
后面的两个2分别表示KV块和FP16精度大小。
举个栗子，对于llama7B，hiddensize = 4096，seqlength = 2048 ， batchsize = 64，layers = 32 计算得到

Memory=4096 X 2048 X 64 X 32 X 2 X 2 =64G

batch_size取1，则只占1GB

### MQA和GQA优化KV cache缓存占用
核心思想：共享多头KV
![[Pasted image 20240826122218.png](image/Pasted%20image%2020240826122218.png)

原公式：Memory = Batch_size X Seq_length X embedding_size X num_layers X 2 X 2
改写后：Memory = Batch_size X Seq_length X head_num X head_size X num_layers X 2 X 2
通过减少KV头的数目，等比例缩减KV cache的显存占用

# LoRA和QLoRA显存占用分析

### LoRA
参数量从 dXd 降低到 2dr，其中r远小于d
假设一个模型不使用混合精度，全程为BF16半精度模型，使用AdamW优化器训练，参数大小为$\Phi$:
+ 模型本身$2\Phi$，旁路lora参数量小于模型2个数量级，忽略不计
+ 优化器只针对需要更新部分处理，但lora太小，也忽略
+ 梯度部分，不需要计算原始模型的梯度，也不占用显存，只有lora部分需要，也可以忽略
+ 合计2$\Phi$
作为参考，llama的训练所需参数量：
![[Pasted image 20240827001633.png](image/Pasted%20image%2020240827001633.png)


### QLoRA
参数分为：
+ 计算参数（参与前向、反向传播的参数）
+ 存储参数（不参与计算，一开始就加载好的参数

QLoRA核心思想：
+ 加载并将16bit参数量化为4bit作为存储参数，在需要计算时反量化为16bit参与计算
+ 所以一般QLoRA的训练时间比LoRA长30%左右

核心创新点（具体见[[SFT微调/常见PEFT方法.md]]）：
+ NF4量化，原始假设参数是均匀分布，该方法假设参数是正态分布，提高了量化精度
+ 双重量化，对于第一次量化得到的锚点参数，进一步进行第二次量化
+ 优化分页器，在GPU紧张时，使用CPU加载参数

占用显存：
+ 16bit LoRA为$2\Phi$，则4bit的QLoRA为$0.5\Phi$


# 总结


| 类别         | 全量微调（FP32/FP16混合精度）                  | 全量微调（BF16） | LoRA    | QLoRA     |
| ---------- | ------------------------------------ | ---------- | ------- | --------- |
| 模型占用       | $4\Phi+2\Phi=6\Phi$（FP32精度+FP16精度模型） | $2\Phi$    | $2\Phi$ | $0.5\Phi$ |
| 优化器（AdamW） | $4\Phi+4\Phi=8\Phi$（一阶、二阶FP32中间梯度）   | $4\Phi$    |         |           |
| 梯度         | $2\Phi$（FP16）                        | $2\Phi$    |         |           |
| 合计         | $16\Phi$                             | $8\Phi$    | $2\Phi$ | $0.5\Phi$ |

