# GPT-1（Generative Pre-Training）
前人问题：
1. 需要大量标注数据
2. 任务难以泛化
GPT思想：
1. 在无标签数据上学习通用模型，再根据特定任务进行微调

无监督预训练：
1. 预测下一个token实现无监督训练
2. 12个transformer，每个都是一个自注意力模块，然后用全连接得到输出概率分布
有监督微调：
1. 任务预测结果损失+预测下一个token损失
2. 对于不同任务的微调方式：
	![[Pasted image 20240806095321.png](../img/Pasted%20image%2020240806095321.png)

# GPT-2
+ 使用了更大的模型规模和更多数据进行预训练
+ 展示了随着模型尺寸的增大，模型的性能也在不断提升

# GPT-3
+ **提出 In-Context Learning ，在下游任务中模型不需要任何额外的微调，利用 Prompts 给定少量标注的样本让模型学习再进行推理生成**。
+ 如何使一个预训练语言模型具有迁移学习能力，在只有少量标注数据的情况下，快速适应新的任务
+ 参数是GPT-2的100倍以上
+ 没有尝试fine-tuning，避免在未训练过的任务上表现差
+ 采用96层多头transformer

# InstructGPT
+ 单纯的大并不意味着可以很好理解用户意图，可能生成不真实、有毒或对用户毫无意义的输出（与用户不一致）
+ 使用来自人类反馈的强化学习方案RLHF对大预言模型微调
![[Pasted image 20240417203943.png](attach/Pasted%20image%2020240417203943.png)
+ 步骤如下：
	+ 定义指令集合
	+ 用InstructGPT生成一个或多个备选指令，供人类评估
	+ 人类对生成备选指令评估，提供一个奖励信号，指示匹配程度
	+ 使用强化学习方法，将生成指令和人类反馈作为训练数据，迭代训练模型，最大化生成的奖励信号