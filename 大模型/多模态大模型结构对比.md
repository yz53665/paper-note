
## 模型整体结构对比

| 模型                       | vision Encoder              | 视觉token | adaptor        | LLM                      | 备注                  |
| ------------------------ | --------------------------- | ------- | -------------- | ------------------------ | ------------------- |
| InternLM-Xcomposer       | EVA-CLIP(1B)                | 257->64 | 类似Q-Former     | InternLLM-chat-7B        |                     |
| InternLM-Xcomposer2      | openai/Clip-vit-large-patch | 4096    | MLPx2          | InternLM2-7B-Chat        | PLoRA               |
| InternLM-XComposer2-4KHD |                             |         |                |                          | 高清图动态切分+换行符         |
| InternLM-XComposer-2.5   |                             |         |                |                          | 支持视频(视频抽帧拼接大图+每帧小图) |
| CogVLM                   | EVA2-CLIP-E                 |         | MLPx2          | Vicuna1.5-7B             |                     |
| CogVLM2                  |                             |         | DownSample+MLP | Meta-Llama-3-8B-Instruct |                     |
| GLM-4V                   |                             |         |                | GLM-4V-9B                |                     |
| Qwen-VL-chat             | CLIP ViT-bigG/14            |         | 类Q-Former      | Qwen-7B                  |                     |


## 模型 Projector对比

| projecter    | 结构                                                                         |
| ------------ | -------------------------------------------------------------------------- |
| MLP          | 直接映射                                                                       |
| Q-Former     | 可学习固定长度短query，特征作为key和value，通过mask控制可见范围，实现不同子任务，用match和对比学习作为辅助任务，单独训练该模块 |
| Transformer  | 可学习的短query，qwenVL使用                                                        |
| CNN          |                                                                            |
| TokenPatcher | downsample和正常的token两条路，做point-region attention，相比transformer，可以更好保留空间结构信息  |

