## 主要思想
+ 将图像看作外语
+ 将视觉为中心的任务与可以使用语言指令灵活定义和管理的语言任务对齐
+ 使用语言指令统一灵活定义所有任务，并使用一个共享的基于llm的任务解码器来解决这些任务

## 模型
#### 1. 总体架构
![[Pasted image 20240417225100.png](attach/Pasted%20image%2020240417225100.png)

> [!NOTE] 图片
> 本质就是魔改的BLIP2

+ 给定描述当前任务的语言指令和输入图像，首相用Language-Guided Image Tokenizer根据给定的提示对图像标记进行编码
+ image tokens 和language instructions倍提供给一个基于llm的开放式任务解码器
+ 最后根据Unified Language Instruction给出的任务定义评估生成的输出，使得模型能够产生特定于任务的结果

#### 通过语言指令——Unified Language Instruction
**用于描述视觉任务，实现视觉和视觉语言任务描述的统一**
如目标检测和实例分割，以一种统一的输出格式：元组（C,P）；其中其中C表示类别集中的类索引，P={xi,yi}表示定位对象的n个点。
``范例：“将所有category set <class>的对象在image的<range>中分割，并生成一个格式为(c, x1, y1, x2, y2，…， x8, y8)。其中，C表示从0开始的类标签的索引，(x1，y1，x2，y2，…，x8，y8)对应于对象边界点相对于中心点的偏移量。 图像是:<Image>”

#### 语言引导的图像分词器——Language-Guided Image Tokenizer
+ 将images视为一种外语并转换为token
+ 灵活针对不同视觉任务编码视觉信息

具体步骤如下：
1. 将图像送到主干ResNet，并提取4个不同尺度视觉特征$f_v$
2. 利用文本编码器Bert从给定的提示中提取语言特征$f_t$
3. 通过交叉注意力将语言特征$f_t$注入到视觉特征的每个尺度中，产生多尺度的语言感知视觉特征，实现跨模态特征对齐
4. 提出采用基于Transformer的网络，使用M个随机初始化的查询项来捕捉图像高层信息（DETR思想），并提取M个图像标记（embedding和位置），用于标记语义和位置

> [!NOTE] 巧妙的利用DETR
> 多尺度的图片特征使得图像可以独立于输入分辨率，并且可以提取于语言提示相关的有信息量的视觉表示


#### 基于LLM的开放式任务解码器——LLM-based Open-Ended Task Decoder
+ 引入\[-512,512\]的离散图像标记
+ 引入位置分类标记，并在语言指令类别集中灵活提供类别和分类标记之间的映射
+ 引入OutputFormat-as Query解码
	+ ![[Pasted image 20240417233813.png](attach/Pasted%20image%2020240417233813.png)
	+ 使用语言模型解析指令中的结构化输出格式（如各类\<cls\>），然后将结构化的输出格式的标记作为插叙输入给解码器，根据插叙生成所需输出

#### 训练过程
+ 一阶段集中于目标检测任务，训练主干网络和语言引导的图像分词器
+ 二阶段引入多任务统一监督，冻结视觉主干

##### 1. 一阶段
+ 初始化模型
+ 训练目标：目标检测任务，使用随机目标类别和任务描述。训练目标时主干网络和语言引导的图像分词器，冻结LLM大部分参数，只训练少量LoRA参数
+ 训练设置：目标检测任务
+ 优化器：AdamW，每个GPU一个样本
##### 2. 二阶段
+ 冻结主干视觉网络
+ 引入多任务统一监督