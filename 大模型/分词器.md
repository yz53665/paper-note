
# 分词粒度

#### word
```text
中文句子：我喜欢看电影和读书。
分词结果：我 | 喜欢 | 看 | 电影 | 和 | 读书。
英文句子：I enjoy watching movies and reading books.
分词结果：I | enjoy | watching | movies | and | reading | books.
```
优点：
+ 语义明确：可以更好保留每个词的含义
+ 上下文理解：有利于保留词语之间的关联性，在语义分析时可以更好捕捉句子意图
缺点：
+ 长尾问题和稀有词语：包含不常见词汇，增加训练和存储成本
+ OOV（Out-of-Vocabulary）：无法处理词表之外的词语
+ 形态和词缀：无法捕捉同一词的不同形态，也无法学习词缀在不同词汇之间的共通性，比如love和loves属于两个词

#### char
```text
中文句子：我喜欢看电影和读书。
分词结果：我 | 喜 | 欢 | 看 | 电 | 影 | 和 | 读 | 书 | 。

英文句子：I enjoy watching movies and reading books.
分词结果：I |   | e | n | j | o | y |   | w | a | t | c | h | i | n | g |   | m | o | v | i | e | s |   | a | n | d |   | r | e | a | d | i | n | g |   | b | o | o | k | s | .
```
优点：
+ 统一的处理方式：适用于不同语言，无需针对设计不同分词规则或工具
+ 避免OOV问题：可以处理任何字符，无需维护词表，适合创新词汇和专有名词

缺点：
+ 语义信息不准确：无法表达词的含义，可能导致在一些语义分析任务中表现较差
+ 处理效率低：粒度太小，增加后续计算时间和成本

#### subword
在BERT时代广泛应用。不希望粒度太大或者太小，希望是介于两者之间的子词单元。包括WordPiece、BPE、BBPE三种方法。其中BPE被应用于GPT，BBPE则被应用于GPT2、Llama等模型上。

# WordPiece（2016）

核心思想：将单词拆分成多个前缀符号的最小单元，再基于词频信息将最小单元合并。例如word可以拆分为：
```
w ##o ##r ##d
```
具体步骤：
1. 计算初始词表及其词频
2. 计算两个最小单元合并后的互信息
3. 选取合并分数最高的进行合并
4. 重复步骤2-3，直到达到要求的词表大小

选取的核心思想：提升概率最大的组合。
句子的似然值：
$$
\log P(S) = \sum_{i=0}^n \log P(t_i)
$$
则合并两个词x和y后组成z，此时的似然值变化：
$$
\log P(t_z) - (\log P(t_x) + \log P(t_y)) = \log \frac{P(t_z)}{P(t_x)P(t_t)}
$$
即合并前后的互信息。
# Byte-pair Encoding(BPE)（1994）

核心思想：合并频率最高的子词，而不计算互信息

缺点：
+ 理论上还是会出现OOV
# Byte-level BPE(BBPE)

特点：理论上不会出现OOV的问题

核心思想：最小词汇单元是字节级别

步骤和BPE一样，只是在构建词表时，不按照字符来构建，而是把字符拆解成bytes作为最小单元
1Bytes = 8bit

# Unigram Language Model(ULM)

WordPiece、BPE的思想都是基于语料库统计，由小到大构建词表。ULM则从方便生成式模型的角度考虑构建词表。
核心思想：最优的分词结果使得句子的似然值最大。

对于一个句子S，它的一种分词方式x的似然值：
$$
P(x) = \prod_{i=0}^{t} P(x_i)
$$
最优的分词方法$x^*$:
$$
x^* = \arg\max_{x\in U(x)} P(x)
$$
用EM算法求解极大似然函数得到每个子词的概率这里的D指语料库：
$$
L=\sum_{s=1}^{|D|} \log P(X^{(s)})
$$
ULM算法步骤通过不断迭代来构造词表：
1. 一般用BPE初始化
2. EM算法求解每个子词在语料上的概率
3. 对于每个子词，计算移除后总体loss降低了多少，记为该子词的loss
4. 丢弃一定比例loss最小的子词，注意为了避免OOV，单字符不丢
5. 重复2-4，直到获得指定大小的词表
ULM会保留以较高频率出现在语料中的子词，避免丢弃后造成较大损失