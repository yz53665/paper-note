
## 核心
如何有效将LLMs与其它模态信息进行结合。整体结构如下：
![[Pasted image 20240415222133.png](attach/Pasted%20image%2020240415222133.png)

主要包含两个阶段：
+ 多模态理解
	+ 模态编码
	+ 输入映射
	+ 大模型主干
+ 多模态生成
	+ 输出映射
	+ 多模态生成

## 多模态编码器

#### 图像
常用的有NFNet-F6、VIT、CLIP VIT、Eva-CLIP VIT

#### 输入投影
将编码得到的特征向量对齐到大模型擅长的文本模态空间中。
一般会有prompt{x, t}，x是多模态数据，t是文本，目的是最小化生成的t的loss

通常可以用MLP、Crossattention

#### 大模型主干
主要产生直接的文本输出，指导多模态生成器生成多模态信号

#### 输出投影
映射到多模态生成器的特征空间

#### 多模态生成器
SD等

## 多模态大模型训练

#### 1. MM PT
预训练，使用X-Text数据集（多模态-文本），只训练输入输出映射，冻结LLM

#### 2. MM IT
有监督IT和强化学习来自人类监督。对齐人类意图，加强交互能力。
将PT的任务转换为instruction模式

## SOTA MM-LLM模型
